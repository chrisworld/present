------------------------
han2015
------------------------

Paper to present:
Song Han, Jeff Pool, John Tran, William J. Dally. Learning both Weights and Connections for Efficient Neural Networks

Hashed Nets:
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. 
Compressing neural networks with the hashing trick

Dropout
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 
Dropout: A simple way to prevent neural networks from overfitting.

inperfect software
http://coderzen.blogspot.co.at/2015/01/synaptic-pruning-in-artificial-neural.html

creativity:
http://imagination-engines.com/iei_founder.php

dog knight:
https://nerdist.com/why-are-googles-neural-networks-making-these-brain-melting-images/

regularization:
https://www.quora.com/What-are-the-main-regularization-methods-used-in-machine-learning

Alex Net:
http://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/

White shark meme:
https://thescinder.com/tag/machine-learning/

killing Machine Learning:
https://www.analyticsvidhya.com/blog/2015/12/hilarious-jokes-videos-statistics-data-science/

Mnist:
https://qph.ec.quoracdn.net/main-qimg-d01751bdf7dab3d9a5949f226a35b7ba

Imagenet:
https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/

TitanX:
http://assets.nvidia.com/nv/pascal//images/titanx-og-hero.jpg